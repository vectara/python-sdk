{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started - Lab 01 - Vectara Index API\n",
    "\n",
    "We'll now explore the Vectara Index API.\n",
    "\n",
    "This notebook will use our \"lab\" authentication profile, if you haven't set this up, please [Setup Authentication](./00_setup_authentication.ipynb).\n",
    "\n",
    "TODO - Insert diagram with Index API highlighted."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f441e6088132a43b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from vectara.factory import Factory\n",
    "from vectara.managers import CreateCorpusRequest\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s:%(name)-35s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S %z')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "client = Factory(profile=\"lab\").build()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f140f3eda6cb94ed",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Corpus and Data\n",
    "We will setup a lab corpus below before we ingest our data. We'll examine this in more depth in the following notebooks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc773cdb40c2925"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "request = CreateCorpusRequest(name=\"Getting Started - Index API\", key=\"02-getting-started-index-api\")\n",
    "response = client.lab_helper.create_lab_corpus(request)\n",
    "\n",
    "logger.info(f\"Our corpus key is [{response.key}]\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7c3aa46eda39740",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Our Content\n",
    "We'll now use the same example as the last lab, loading Shakespeare's _Taming of the Shrew_ text.\n",
    "\n",
    "It's important to note that this text is \"one block\". Dependent on which method we use below will dictate how it is\n",
    "structured."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f32cc02dd5b162e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"resources/shakespeare/taming_shrew.txt\")\n",
    "logger.info(f\"Loading {path}\")\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    play_text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68cd9f8c1446a32d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Automatic Chunking with Structured Document Indexing\n",
    "We'll now submit the document with the structured document indexing. This is the simplest method to\n",
    "put data in Vectara and works for most use cases with unstructured data. The only downside is that document\n",
    "parts may span multiple sections.\n",
    "\n",
    "We will highlight the important fields on the indexing below:\n",
    "\n",
    "* **id** - each document in a corpus must have a unique id field. You cannot insert a document when an ID already exists and must first delete it.\n",
    "* **type** - for the V2 API, you must provide a type, which may be \"structured\" as per below or \"core\" which we'll show next. This is known as a discriminator value and indicates which type of document you are submitting.\n",
    "* **title** - provided for context which helps the retrieval model and re-ranker determine relevancy to the users query.\n",
    "* **description** - provides further context like the title field.\n",
    "* **sections** - for structured documents, you must provide the sections of text. There are other fields which can be present here in a nested structure, however the \"text\" field may be split into multiple \"document_part\" sections.\n",
    "\n",
    "A key takeaway here is that the \"sections\" field will be transformed by Vectara into the \"core\" document format using \n",
    "optimal processing. This will work for most use cases however you may have requirements that define strict boundaries\n",
    "on the document parts."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba9d3c3020a540da"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from vectara.types import StructuredDocument\n",
    "\n",
    "request = StructuredDocument.parse_obj({\n",
    "   \"id\": \"my-doc\",\n",
    "   \"type\": \"structured\",\n",
    "   \"title\": \"Taming of the Shrew\",\n",
    "   \"description\": \"The Shakespeare play, 'the Taming of the Shrew'\",\n",
    "   \"sections\": [\n",
    "       {\n",
    "           \"text\": play_text # One big section which will be automatically chunked.\n",
    "       }\n",
    "   ]\n",
    "})\n",
    "\n",
    "client.documents.index(response.key, request=request)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c0fc9401b1b28a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from vectara.queries import SearchCorpusParameters\n",
    "from vectara.types import GenerationParameters, ContextConfiguration\n",
    "import json\n",
    "\n",
    "query = \"Does Sly offer to pay for the broken glasses?\"\n",
    "\n",
    "search_corpus = SearchCorpusParameters.parse_obj({\n",
    "    # TODO Add reranker from SearchParameters#SearchReranker\n",
    "    # TODO Add context_configuration from SearchParameters#ContextConfiguration\n",
    "    \"limit\": 1,\n",
    "    \"context_configuration\": {\n",
    "        \"characters_before\": 10000,\n",
    "        \"characters_after\": 10000,\n",
    "        \"start_tag\": \"<b>\",\n",
    "        \"end_tag\": \"</b>\"\n",
    "    }\n",
    "    \n",
    "})\n",
    "\n",
    "query_response = client.queries.query_corpus(response.key, query=query, search=search_corpus)\n",
    "logger.info(json.dumps(query_response.model_dump(), indent=4))\n",
    "\n",
    "logger.info(f\"Document part length is [{len(query_response.search_results[0].text)}]\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e67a57166971311",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some Structuring\n",
    "We can see from the example above that there is no true \"part\" - the document is stored internally as one giant part.\n",
    "\n",
    "The chunking is done automatically behind the scenes.\n",
    "\n",
    "We can break up the document parts into more logical elements. We'll now parse the ingested document into acts (INDUCTION, ACT 1, ACT 2 etc) and scenes (Scene 1, Scene 2).\n",
    "This will allow us to do 2 things:\n",
    "1. Utilise the metadata to target specific sections in the document which will be relevant when we look at Filter Attributes.\n",
    "2. Seperate distinct areas of text and avoid unrelated context between sections (clipping information which should be distinct)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "759602d5c8849c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "acts = []\n",
    "act = {\"name\": \"Overview\", \"scenes\": []}\n",
    "scene_name = None\n",
    "scene_texts = []\n",
    "\n",
    "break_marker = re.compile(r'^=+$')\n",
    "\n",
    "ignored_break_markers = [\n",
    "    \"Characters in the Play\"\n",
    "]\n",
    "\n",
    "scene_prefix = \"Scene \"\n",
    "\n",
    "last = \"\"\n",
    "logger.info(f\"Loading {path}\")\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        #logger.info(f\"{idx:03} Received line: {stripped_line}\")\n",
    "        if idx > 200:\n",
    "            break\n",
    "        if idx > 0:\n",
    "            if break_marker.match(stripped_line):\n",
    "                if last in ignored_break_markers:\n",
    "                    continue\n",
    "                logger.info(f\"Found break marker, last was: [{last}]\")\n",
    "                if stripped_line.startswith(scene_prefix):\n",
    "                    # Put the last scene into the act (if not empty)\n",
    "                    if len(scene_text) > 0:\n",
    "                        scene_text = \"\\n\".join(scene_texts)\n",
    "                        scene = {\n",
    "                            \"text\": scene_text\n",
    "                        }\n",
    "                        if scene_name:\n",
    "                            scene[\"name\"] = scene_name\n",
    "                        act[\"scenes\"].append(scene)\n",
    "                        \n",
    "                        # Reset the scene variables\n",
    "                        scene_texts = []\n",
    "                        scene_name = last\n",
    "                else:\n",
    "                    # New Act.\n",
    "                        \n",
    "                            \n",
    "                        \n",
    "        \n",
    "        last = stripped_line"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afc33232f190d89d",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
