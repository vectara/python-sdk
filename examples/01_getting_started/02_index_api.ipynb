{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f441e6088132a43b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Getting Started - Lab 01 - Vectara Index API\n",
    "\n",
    "We'll now explore the Vectara Index API, where we encode our data into vectors using the Boomerang model which provides\n",
    "the best bi-lingual meaning and intent embeddings in the industry. We then store the embeddings plus the document text\n",
    "and metadata together in the corpus.\n",
    "\n",
    "This notebook will use our \"lab\" authentication profile, if you haven't set this up, please [Setup Authentication](./00_setup_authentication.ipynb).\n",
    "\n",
    "<img src=\"./resources/platform-capabilities-index.png\" alt=\"Platform Capabilities - Encode and Index\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140f3eda6cb94ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from vectara.factory import Factory\n",
    "from getting_started_util import GettingStartedUtil\n",
    "\n",
    "util = GettingStartedUtil()\n",
    "logger = util.logger\n",
    "client = Factory(profile=\"lab\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc773cdb40c2925",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Setup Corpus\n",
    "We will setup a lab corpus below before we ingest our data. We'll examine this in more depth in the following notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3aa46eda39740",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corpus_key = util.setup_02(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32cc02dd5b162e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load Our Content\n",
    "We'll now use the same example as the last lab, loading Shakespeare's _Taming of the Shrew_ text.\n",
    "\n",
    "It's important to note that this text is \"one block\". Dependent on which method we use below will dictate how it is\n",
    "structured.\n",
    "\n",
    "<img src=\"./resources/Taming_of_the_Shrew_01.jpg\" alt=\"Taming of the Shrew\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd9f8c1446a32d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"resources/shakespeare/taming_shrew.txt\")\n",
    "logger.info(f\"Loading {path}\")\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    play_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d3c3020a540da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "## Automatic Chunking with Structured Document Indexing\n",
    "We'll now submit the document with the structured document indexing. This is the simplest method to\n",
    "put data in Vectara and works for most use cases with unstructured data. The only downside is that document\n",
    "parts may span multiple chunks. The current default indexing API chunks at the sentence level, with some caveats.\n",
    "\n",
    "Chunking strategies is an advanced topic - there are lots of pros and cons of different chunking strategies. We're\n",
    "keen to hear feedback - and if you need more control you can look at the CoreIndex method below. If you want to learn\n",
    "more about how we do it at Vectara, see the following blog article: https://vectara.com/blog/grounded-generation-done-right-chunking/ \n",
    "\n",
    "We will highlight the important fields on the indexing below:\n",
    "\n",
    "* **id** - each document in a corpus must have a unique id field. You cannot insert a document when an ID already exists and must first delete it.\n",
    "* **type** - for the V2 API, you must provide a type, which may be \"structured\" as per below or \"core\" which we'll show next. This is known as a discriminator value and indicates which type of document you are submitting.\n",
    "* **title** - provided for context which helps the retrieval model and re-ranker determine relevancy to the users query.\n",
    "* **description** - provides further context like the title field.\n",
    "* **sections** - for structured documents, you must provide the sections of text. There are other fields which can be present here in a nested structure, however the \"text\" field may be split into multiple \"document_part\" sections.\n",
    "\n",
    "A key takeaway here is that the \"sections\" field will be transformed by Vectara into the \"core\" document format using \n",
    "optimal processing. This will work for most use cases however you may have requirements that define strict boundaries\n",
    "on the document parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fc9401b1b28a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from vectara.types import StructuredDocument\n",
    "\n",
    "request = StructuredDocument.parse_obj({\n",
    "   \"id\": \"taming_of_the_shrew_structured\",\n",
    "   \"type\": \"structured\",\n",
    "   \"title\": \"Taming of the Shrew\",\n",
    "   \"description\": \"The Shakespeare play, 'the Taming of the Shrew'\",\n",
    "   \"sections\": [\n",
    "       {\n",
    "           \"text\": play_text # One big section which will be automatically chunked.\n",
    "       }\n",
    "   ]\n",
    "})\n",
    "\n",
    "structured_index_response = client.documents.create(corpus_key, request=request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0bbbf6c85509b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Let's Look at a \"document_part\"\n",
    "In order to know what the Retrieval model can \"see\" when searching for relevant parts, we can take a look at one of the parts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849d939cc053a05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "indexed_doc_resp = client.documents.get_corpus_document(corpus_key, \"taming_of_the_shrew_structured\")\n",
    "\n",
    "logger.info(f\"Here's the 2nd document part:\\n{indexed_doc_resp.parts[100].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295c0114d4d786",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Document Size and Reduction\n",
    "You can see that the ingested size is slightly smaller than the original text. We see\n",
    "about a 97% of the size of it's source, however text and JSON does not reduce much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d18fa81d97a3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_usage_info(index_response_1):\n",
    "    bytes_used = index_response_1.storage_usage.bytes_used\n",
    "    metadata_bytes_used = index_response_1.storage_usage.metadata_bytes_used\n",
    "\n",
    "    original_size = len(bytearray(play_text, \"ascii\"))\n",
    "    reduction_pct = (original_size - bytes_used) / original_size * 100\n",
    "    kb_used = int(bytes_used / 1024)\n",
    "    metadata_kb_used = int(metadata_bytes_used / 1024)\n",
    "    logger.info(f\"The text was reduced by [{reduction_pct:.3}%]\")\n",
    "    logger.info(f\"Total data storage is [{kb_used}KB]\")\n",
    "    logger.info(f\"Total metadata storage is [{metadata_kb_used}KB]\")\n",
    "\n",
    "show_usage_info(structured_index_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67a57166971311",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from vectara.corpora import SearchCorpusParameters\n",
    "from vectara.types import GenerationParameters, ContextConfiguration\n",
    "import json\n",
    "\n",
    "def run_query(doc_id):\n",
    "    query = \"Does Sly offer to pay for the broken glasses?\"\n",
    "    \n",
    "    generation = GenerationParameters.parse_obj({\n",
    "        \"generation_preset_name\": \"vectara-summary-ext-v1.3.0\",\n",
    "        \"max_used_search_results\": 5,\n",
    "        \"max_response_characters\": 300,\n",
    "        \"response_language\": \"auto\",\n",
    "        \n",
    "    })\n",
    "    \n",
    "    search_corpus = SearchCorpusParameters.parse_obj({\n",
    "        \"lexical_interpolation\": 0.025,\n",
    "        \"semantics\": \"default\",\n",
    "        \"offset\": 0,\n",
    "        \"limit\": 10,\n",
    "        \"reranker\": {\n",
    "            \"type\": \"customer_reranker\",\n",
    "            \"reranker_id\": \"rnk_272725719\" # Multi-lingual Re-Ranker\n",
    "        },\n",
    "        \"context_configuration\": {\n",
    "            \"characters_before\": 30,\n",
    "            \"characters_after\": 30,\n",
    "            \"start_tag\": \"<b>\",\n",
    "            \"end_tag\": \"</b>\"\n",
    "        },\n",
    "    })\n",
    "    \n",
    "    query_response = client.corpora.query(corpus_key, query=query, search=search_corpus, generation=generation)\n",
    "    logger.info(f\"Document summary for document with id [{doc_id}] is [{query_response.summary}]\")\n",
    "    return query_response.summary\n",
    "\n",
    "structured_summary = run_query(\"taming_of_the_shrew_structured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759602d5c8849c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Some Structuring\n",
    "We can see from the example above that there is no true \"part\" - the document is stored internally as one giant part.\n",
    "\n",
    "The chunking is done automatically behind the scenes.\n",
    "\n",
    "We can break up the document parts into more logical elements. We'll now parse the ingested document into acts (INDUCTION, ACT 1, ACT 2 etc) and scenes (Scene 1, Scene 2).\n",
    "This will allow us to do 2 things:\n",
    "1. Utilise the metadata to target specific sections in the document which will be relevant when we look at Filter Attributes.\n",
    "2. Seperate distinct areas of text and avoid unrelated context between sections (clipping information which should be distinct).\n",
    "\n",
    "Note - we'll use an extension of this example to add metadata for the Scene and Act to the information when we perform Corpus Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc33232f190d89d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# You can ignore the code here - we use the Act/Scene breaks in the text file as section delimiters.\n",
    "acts = util.lab_02_chunk_play(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d269f564af86c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Convert to Chunks and Index\n",
    "After we've extracted the text from the raw content, we break apart into sub-chunks at a 1000 character limit with a 50 character overlap.\n",
    "\n",
    "Once done, we index the resulting document with the same API call as we used for the Structured document. The key difference is that we've\n",
    "manually created our chunks and specify \"core\" instead of \"structured\".\n",
    "\n",
    "You will also notice we add in metadata we extracted from the document for the Act and Scene - allowing us to ask very specific questions\n",
    "when combined with Corpus Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ab752c8b356ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "document_parts = []\n",
    "core_document = {\n",
    "    \"id\": \"taming_of_the_shrew_core\",\n",
    "    \"type\": \"core\",\n",
    "    \"document_parts\": document_parts # Add these in the loop below.\n",
    "}\n",
    "\n",
    "for act in acts:\n",
    "    act_name = act[\"name\"]\n",
    "    logger.info(f\"Act: {act_name}\")\n",
    "    \n",
    "    for scene in act[\"scenes\"]:\n",
    "        scene_name = scene[\"name\"]\n",
    "        logger.info(f\"\\tScene: {scene_name}\")\n",
    "        \n",
    "        # Add a title\n",
    "        scene_title_part = {\n",
    "            \"text\": f\"{act_name} - {scene_name}\",\n",
    "            \"metadata\": {\n",
    "                \"is_title\": True,\n",
    "                \"act\": act[\"name\"],\n",
    "                \"scene\": scene[\"name\"]\n",
    "            }\n",
    "        }\n",
    "        document_parts.append(scene_title_part)\n",
    "        \n",
    "        full_text = \"\\n\".join(scene[\"scene_texts\"])\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(full_text):\n",
    "            \n",
    "            # Simple chunks at 1000 characters with a 50 character overlap.\n",
    "            chunk_text = full_text[start:start+1000]\n",
    "            scene_chunk_part = {\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"act\": act[\"name\"],\n",
    "                    \"scene\": scene[\"name\"]\n",
    "                }\n",
    "            }\n",
    "            document_parts.append(scene_chunk_part)\n",
    "            start += 950\n",
    "\n",
    "core_index_response = client.documents.create(corpus_key, request=core_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fae4859da23f73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Check the Core Document\n",
    "Now we'll investigate the the core document parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14010374a2783d0e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "indexed_doc_resp = client.documents.get_corpus_document(corpus_key, \"taming_of_the_shrew_core\")\n",
    "\n",
    "logger.info(f\"Here's the 2nd document part:\\n{indexed_doc_resp.parts[100].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831c2595ca40159",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Remember, here was our first summary:\\n{structured_summary}\")\n",
    "\n",
    "core_summary = run_query(\"taming_of_the_shrew_core\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
